{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb8bdeb",
   "metadata": {},
   "source": [
    "## Staging and Updating DBs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f07a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "# # Live DB (read-only)\n",
    "# live_client = MongoClient(\"mongodb://username:password@host:port/\")\n",
    "# live_db = live_client[\"milk_erp_live\"]\n",
    "\n",
    "uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "def explore_cluster(client):\n",
    "    from pprint import pprint\n",
    "\n",
    "    db_names = client.list_database_names()\n",
    "    if not db_names:\n",
    "        print(\"No databases found\")\n",
    "        return\n",
    "\n",
    "    for db_name in db_names:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Database: {db_name}\".upper())\n",
    "        print(\"=\" * 60)\n",
    "        db = client[db_name]\n",
    "        collections_names = db.list_collection_names()\n",
    "        if not collections_names:\n",
    "            print(\"  No collections found.\\n\")\n",
    "        else:\n",
    "            for col_name in collections_names:\n",
    "                print(\"\\n\" + \"-\" * 40)\n",
    "                print(f\"  Collection: {col_name}\")\n",
    "                print(\"-\" * 40)\n",
    "                collection = db[col_name]\n",
    "                sample_docs = collection.find().limit(3)\n",
    "                for i, doc in enumerate(sample_docs, 1):\n",
    "                    print(f\"    Sample Document {i}:\")\n",
    "                    pprint(doc, indent=8, width=120)\n",
    "                    print()\n",
    "        print(\"\\n\")\n",
    "                        \n",
    "explore_cluster(client)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b5ed4",
   "metadata": {},
   "source": [
    "Update Collection function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfe6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_old= client[\"milk_erp_dummy_copy\"]\n",
    "db_new = client[\"milk_erp_new\"]\n",
    "\n",
    "\n",
    "old_purchases=db_old[\"purchases\"]\n",
    "new_purchases=db_new[\"purchases\"]\n",
    "\n",
    "def update_collection(old_coll, new_coll): \n",
    "    \n",
    "    last_doc = old_coll.find_one(sort=[(\"_id\", -1)])\n",
    "    last_id = last_doc[\"_id\"] if last_doc else None\n",
    "\n",
    "    query= {\"_id\": {\"$gt\": last_id}} if last_id else {}\n",
    "    new_docs = list(new_coll.find(query))\n",
    "\n",
    "    if new_docs: \n",
    "        old_coll.insert_many(new_docs)\n",
    "        print(f\"Inserted {len(new_docs)} new documents.\")\n",
    "    else:\n",
    "        print(\"No new documents to insert.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc3b42",
   "metadata": {},
   "source": [
    "Update Database Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(db_old, db_new, collections):\n",
    "    for coll_name in collections:\n",
    "        old_coll = db_old[coll_name]\n",
    "        new_coll = db_new[coll_name]\n",
    "\n",
    "        last_doc = old_coll.find_one(sort=[(\"_id\", -1)])\n",
    "        last_id = last_doc[\"_id\"] if last_doc else None\n",
    "\n",
    "        query = {\"_id\": {\"$gt\": last_id}} if last_id else {}\n",
    "        new_docs = list(new_coll.find(query))\n",
    "\n",
    "        if new_docs:\n",
    "            old_coll.insert_many(new_docs)\n",
    "            print(f\"[{coll_name}] Inserted {len(new_docs)} new docs\")\n",
    "        else:\n",
    "            print(f\"[{coll_name}] No new docs\")\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a01fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## code for copying a db. \n",
    "# source_db = client[\"milk_erp_dummy\"]\n",
    "# target_db = client[\"milk_erp_dummy_copy\"]\n",
    "\n",
    "# # Copy all collections\n",
    "# for coll_name in source_db.list_collection_names():\n",
    "#     source_coll = source_db[coll_name]\n",
    "#     target_coll = target_db[coll_name]\n",
    "    \n",
    "#     docs = list(source_coll.find({}))\n",
    "#     if docs:\n",
    "#         target_coll.insert_many(docs)\n",
    "\n",
    "# print(\"Database copied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_database(db_old, db_new, db_new.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5412d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import bson\n",
    "from bson import ObjectId\n",
    "from pymongo import UpdateOne\n",
    "\n",
    "# -------------------------------\n",
    "# DB CONNECTIONS\n",
    "# -------------------------------\n",
    "LIVE_CONN_STR = \"mongodb://localhost:27017\"\n",
    "REPORTING_CONN_STR = \"mongodb://localhost:27017\"\n",
    "\n",
    "live_client = MongoClient(LIVE_CONN_STR)\n",
    "reporting_client = MongoClient(REPORTING_CONN_STR)\n",
    "\n",
    "live_db = live_client[\"milk_erp_new\"]\n",
    "reporting_db = reporting_client[\"staging_db\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee33bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# HELPER: Get last run timestamp\n",
    "# -------------------------------\n",
    "def to_objectid_safe(x):\n",
    "        if pd.isnull(x):\n",
    "            return None\n",
    "        if isinstance(x, ObjectId):\n",
    "            return x\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ObjectId(x)\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None \n",
    "    \n",
    "from datetime import datetime\n",
    "def sanitize_datetimes(df):\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: (\n",
    "                    x.to_pydatetime() if isinstance(x, pd.Timestamp) and pd.notna(x)\n",
    "                    else (x if isinstance(x, datetime) else None)\n",
    "                )\n",
    "            )\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_last_run(collection_name):\n",
    "    meta = reporting_db[\"etl_metadata\"].find_one({\"_id\": collection_name})\n",
    "    if meta and \"last_run\" in meta:\n",
    "        ts = meta[\"last_run\"]\n",
    "        # Ensure it is a datetime, not pandas Timestamp\n",
    "        if isinstance(ts, pd.Timestamp):\n",
    "            return ts.to_pydatetime()\n",
    "        return ts\n",
    "    return None\n",
    "\n",
    "def update_last_run(timestamp, collection_name):\n",
    "    if pd.isna(timestamp):\n",
    "        safe_ts = None\n",
    "    elif isinstance(timestamp, pd.Timestamp):\n",
    "        safe_ts = timestamp.to_pydatetime()\n",
    "    elif isinstance(timestamp, datetime):\n",
    "        safe_ts = timestamp\n",
    "    else:\n",
    "        safe_ts = None\n",
    "\n",
    "    reporting_db[\"etl_metadata\"].update_one(\n",
    "        {\"_id\": collection_name},\n",
    "        {\"$set\": {\"last_run\": safe_ts}},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    \n",
    "def extract_test_values(tests, target_name):\n",
    "    \"\"\"Helper to pull test value for a specific qa_test_name if status==1.\"\"\"\n",
    "    if not tests:\n",
    "        return None\n",
    "    for t in tests:\n",
    "        if t.get(\"qa_test_name\") == target_name and t.get(\"status\") == 1:\n",
    "            return t.get(\"value\")\n",
    "    return None\n",
    "\n",
    "def find_base_price(purchase, prices_df, arch_prices_df):\n",
    "    return None\n",
    "\n",
    "def find_plant_base_price(purchase, prices_df, arch_prices_df):\n",
    "    return \n",
    "\n",
    "# -------------------------------\n",
    "# EXTRACT: Incremental from live DB\n",
    "# -------------------------------\n",
    "def extract_incremental_purchases(db, collection_name, last_run):\n",
    "    if last_run:\n",
    "        query = {\"created_at\": {\"$gte\": last_run}}\n",
    "    else:\n",
    "        query = {}  # First full load\n",
    "\n",
    "    fields = {\n",
    "        \"_id\": 1,\n",
    "        \"supplier_id\": 1,\n",
    "        \"supplier_type_id\": 1,\n",
    "        \"mcc_id\": 1,\n",
    "        \"area_office_id\": 1,\n",
    "        \"gross_volume\": 1,\n",
    "        \"ts_volume\": 1,\n",
    "        \"opening_balance\": 1,\n",
    "        \"type\": 1,\n",
    "        \"created_by\": 1,\n",
    "        \"created_at\": 1,\n",
    "        \"booked_at\": 1, \n",
    "        \"time\": 1,\n",
    "        \"serial_number\" : 1, \n",
    "        \"is_planned\": 1, \n",
    "        \"is_exceptional_release\": 1,\n",
    "        \"tests\": 1,\n",
    "        \"plant_id\": 1,\n",
    "        \"price\": 1\n",
    "        \n",
    "    }\n",
    "    cursor = db[collection_name].find(query, fields)\n",
    "    cursor = db[collection_name].find(query, fields)\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Ensure expected columns always exist\n",
    "    for col in [\"mcc_id\", \"supplier_id\", \"supplier_type_id\", \"area_office_id\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "# -------------------------------\n",
    "# TRANSFORM: Merge with lookups\n",
    "# -------------------------------\n",
    "def transform_purchases(purchases_df):\n",
    "    if purchases_df.empty:\n",
    "        return purchases_df\n",
    "    \n",
    "    purchases_df[\"created_at\"] = pd.to_datetime(purchases_df[\"created_at\"], errors=\"coerce\")\n",
    "    purchases_df[\"booked_at\"] = pd.to_datetime(purchases_df[\"booked_at\"], errors=\"ignore\")\n",
    "    purchases_df[\"time\"] = pd.to_datetime(purchases_df[\"time\"], errors=\"coerce\")\n",
    "\n",
    "     # Replace null/NaT booked_at with values from 'time' column (if present)\n",
    "    if \"booked_at\" in purchases_df.columns and \"time\" in purchases_df.columns:\n",
    "        null_mask = purchases_df[\"booked_at\"].isna()\n",
    "        if null_mask.any():\n",
    "            print(f\"ℹ️ Replacing {int(null_mask.sum())} null booked_at values with time\")\n",
    "            purchases_df.loc[null_mask, \"booked_at\"] = purchases_df.loc[null_mask, \"time\"]\n",
    "\n",
    "\n",
    "    suppliers_df = pd.DataFrame(list(live_db[\"suppliers\"].find({})))\n",
    "    collection_points_df = pd.DataFrame(list(live_db[\"collection_points\"].find({}, {\n",
    "        \"_id\": 1, \"name\": 1, \"area_office_id\": 1, \"status\": 1,\n",
    "        \"is_mcc\": 1, \"latitude\": 1, \"longitude\": 1, \"address\": 1\n",
    "    })))\n",
    "    area_offices_df = pd.DataFrame(list(live_db[\"area_offices\"].find({}, {\"_id\": 1, \"name\": 1})))\n",
    "    supplier_types_df = pd.DataFrame(list(live_db[\"supplier_types\"].find({}, {\"_id\": 1, \"name\": 1, \"description\": 1})))\n",
    "\n",
    "    # Convert IDs\n",
    "    purchases_df[\"_id\"] = purchases_df[\"_id\"].apply(to_objectid_safe)\n",
    "    suppliers_df[\"_id\"] = suppliers_df[\"_id\"].apply(to_objectid_safe)\n",
    "    collection_points_df[\"_id\"] = collection_points_df[\"_id\"].apply(to_objectid_safe)\n",
    "    collection_points_df[\"area_office_id\"] = collection_points_df[\"area_office_id\"].apply(to_objectid_safe)\n",
    "    area_offices_df[\"_id\"] = area_offices_df[\"_id\"].apply(to_objectid_safe)\n",
    "    supplier_types_df[\"_id\"] = supplier_types_df[\"_id\"].apply(to_objectid_safe)\n",
    "\n",
    "    suppliers_df[\"supplier_type_id\"] = suppliers_df[\"supplier_type_id\"].apply(to_objectid_safe)\n",
    "    purchases_df[\"supplier_id\"] = purchases_df[\"supplier_id\"].apply(to_objectid_safe)\n",
    "\n",
    "    purchases_df[\"mcc_id\"] = purchases_df[\"mcc_id\"].apply(to_objectid_safe)\n",
    "    purchases_df[\"supplier_type_id\"] = purchases_df[\"supplier_type_id\"].apply(to_objectid_safe)\n",
    "\n",
    "    # Select relevant columns\n",
    "    suppliers_df  = suppliers_df[[\"_id\", \"name\", \"supplier_type_id\", \"source\", \"area_office\", \"code\"]]\n",
    "    collection_points_df = collection_points_df[[\"_id\", \"name\", \"area_office_id\", \"is_mcc\", \"latitude\", \"longitude\"]]\n",
    "    area_offices_df = area_offices_df[[\"_id\", \"name\"]]\n",
    "    supplier_types_df = supplier_types_df[[\"_id\", \"name\", \"description\"]]\n",
    "\n",
    "    # Joins\n",
    "    purchases_df = purchases_df.merge(\n",
    "        suppliers_df.rename(columns={\"_id\": \"supplier_id\", \"name\": \"supplier_name\"}),\n",
    "        on=\"supplier_id\", how=\"left\", suffixes=(\"\", \"_sup\")\n",
    "    )\n",
    "    print(\"After Suppliers Join:\")\n",
    "    display(purchases_df.dtypes)\n",
    "    \n",
    "    purchases_df = purchases_df.merge(\n",
    "        collection_points_df.rename(columns={\"_id\": \"mcc_id\", \"name\": \"collection_point_name\"}),\n",
    "        on=\"mcc_id\", how=\"left\", suffixes=(\"\", \"_mcc\")\n",
    "    )\n",
    "    print(\"After MCC Join:\")\n",
    "    display(purchases_df.dtypes)\n",
    "\n",
    "    purchases_df = purchases_df.merge(\n",
    "        area_offices_df.rename(columns={\"_id\": \"area_office_id\", \"name\": \"area_office_name\"}),\n",
    "        left_on=\"area_office_id_mcc\",right_on=\"area_office_id\", how=\"left\", suffixes=(\"\", \"_ao\")\n",
    "    )\n",
    "    purchases_df = purchases_df.merge(\n",
    "        supplier_types_df.rename(columns={\"_id\": \"supplier_type_id\", \"name\": \"supplier_type_name\"}),\n",
    "        on=\"supplier_type_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Ensure datetime safe\n",
    "    # purchases_df[\"booked_date\"] = pd.to_datetime(purchases_df[\"booked_date\"], errors=\"coerce\")\n",
    "\n",
    "    \n",
    "    # Drop duplicates of source columns only if they exist\n",
    "    for col in [\"area_office\", \"source\"]:\n",
    "        if col in purchases_df.columns:\n",
    "            purchases_df = purchases_df.drop(columns=[col])\n",
    "\n",
    "    print(f\"transform_purchases → rows={len(purchases_df)}, with _id={purchases_df['_id'].notna().sum()}\")\n",
    "    # display(purchases_df.head(2))\n",
    "    # display(purchases_df.dtypes)\n",
    "    return purchases_df\n",
    "\n",
    "\n",
    "def load_to_reporting(df, collection_name):\n",
    "    if df.empty:\n",
    "        print(\"⚠️ Nothing to load\")\n",
    "        return\n",
    "\n",
    "    df = sanitize_datetimes(df)   # <- NEW LINE\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    # print(df[\"booked_date\"].unique()[:10])\n",
    "    # print(df[\"booked_date\"].dtype)\n",
    "    # df.drop(columns=\"booked_date\", inplace=True, errors='ignore')\n",
    "    # print(\"After sanitizing datetimes:\")\n",
    "    # display(df.dtypes)\n",
    "    # display(df.head(2))\n",
    "\n",
    "    df = df[df[\"_id\"].notnull()]\n",
    "    if df.empty:\n",
    "        print(\"⚠️ All rows missing _id, nothing to upsert\")\n",
    "        return\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\" or \"datetime\" in str(df[col].dtype):\n",
    "            bad = df[col][df[col].apply(lambda x: isinstance(x, pd._libs.tslibs.nattype.NaTType))]\n",
    "            if not bad.empty:\n",
    "                print(f\"⚠️ Column {col} still has NaT values:\", bad.head())\n",
    "\n",
    "    ops = []\n",
    "    for _, row in df.iterrows():\n",
    "        doc = row.to_dict()\n",
    "        _id = doc.pop(\"_id\")\n",
    "        ops.append(UpdateOne({\"_id\": _id}, {\"$set\": doc}, upsert=True))\n",
    "\n",
    "    result = reporting_db[collection_name].bulk_write(ops, ordered=False)\n",
    "    print(f\"✅ Upserted: matched={result.matched_count}, modified={result.modified_count}, upserted={len(result.upserted_ids)}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# MAIN ETL RUN\n",
    "# -------------------------------\n",
    "def run_etl(collection_name):\n",
    "    last_run = get_last_run(collection_name)\n",
    "    print(f\"🔍 Last ETL run: {last_run}\")\n",
    "\n",
    "    # 1. Extract incremental\n",
    "    if collection_name == \"milk_purchases\":\n",
    "        relevant_df = extract_incremental_purchases(live_db, collection_name, last_run)\n",
    "        # display(relevant_df.head(2))\n",
    "        print(f\"📦 Extracted {len(relevant_df)} new/updated purchases\")\n",
    "    else:\n",
    "        print(f\"No extraction defined for collection: {collection_name}\")\n",
    "        relevant_df = pd.DataFrame()\n",
    "\n",
    "    # 2. Transform\n",
    "    if collection_name == \"milk_purchases\":\n",
    "        transformed_df = transform_purchases(relevant_df)\n",
    "        print(f\"🔄 Transformed DF\")\n",
    "        prices_df = pd.DataFrame(list(live_db[\"prices\"].find({\"status\": 1})))\n",
    "        archived_prices_df = pd.DataFrame(list(live_db[\"archieved_prices\"].find({\"status\": 1})))\n",
    "\n",
    "        prices_df[\"plant\"]= prices_df[\"plant\"].apply(to_objectid_safe)\n",
    "        archived_prices_df[\"plant\"]= archived_prices_df[\"plant\"].apply(to_objectid_safe)\n",
    "        prices_df[\"source_type\"]= prices_df[\"source_type\"].apply(to_objectid_safe)\n",
    "        archived_prices_df[\"source_type\"]= archived_prices_df[\"source_type\"].apply(to_objectid_safe)\n",
    "        prices_df[\"area_office\"]= prices_df[\"area_office\"].apply(to_objectid_safe)\n",
    "        archived_prices_df[\"area_office\"]= archived_prices_df[\"area_office\"].apply(to_objectid_safe)\n",
    "        prices_df[\"supplier\"]= prices_df[\"supplier\"].apply(to_objectid_safe)\n",
    "        archived_prices_df[\"supplier\"]= archived_prices_df[\"supplier\"].apply(to_objectid_safe)\n",
    "        prices_df[\"collection_point\"]= prices_df[\"collection_point\"].apply(to_objectid_safe)\n",
    "        archived_prices_df[\"collection_point\"]= archived_prices_df[\"collection_point\"].apply(to_objectid_safe)\n",
    "\n",
    "        print(\"Min/Max WEF in prices:\", prices_df[\"wef\"].min(), prices_df[\"wef\"].max())\n",
    "        print(\"Sample WEF values:\", prices_df[\"wef\"].sort_values().head(10).tolist())\n",
    "        print(\"Sample booked times:\", transformed_df[\"booked_at\"].sort_values().head(10).tolist())\n",
    "        print(\"Min/Max booked_at in purchases:\", transformed_df[\"booked_at\"].min(), transformed_df[\"booked_at\"].max())\n",
    "\n",
    "\n",
    "    \n",
    "        # print(\"example prices:\")\n",
    "        # display(prices_df.head(2))\n",
    "        # display(archived_prices_df.head(2))\n",
    "        \n",
    "        import importlib\n",
    "        import pricing\n",
    "        importlib.reload(pricing)  # Ensure latest version is used\n",
    "        from pricing import attach_prices\n",
    "        transformed_df = attach_prices(transformed_df, prices_df, archived_prices_df)\n",
    "        print(\"💰 Prices attached\")\n",
    "    else: \n",
    "        print(f\"No transformation defined for collection: {collection_name}\")\n",
    "\n",
    "\n",
    "    # 3. Load\n",
    "    # print(\"Before Loading DF: \")\n",
    "    # display(transformed_df.head(2))\n",
    "    # display(transformed_df.dtypes)\n",
    "    \n",
    "    load_to_reporting(transformed_df, \"fact_\"+collection_name)\n",
    "\n",
    "    # 4. Update checkpoint\n",
    "    if not relevant_df.empty:\n",
    "        # relevant_df[\"created_at\"] = pd.to_datetime(relevant_df[\"created_at\"], errors=\"coerce\")\n",
    "        # relevant_df[\"booked_date\"] = pd.to_datetime(relevant_df[\"booked_date\"], errors=\"coerce\")\n",
    "        new_last_run = relevant_df[\"created_at\"].max()\n",
    "        update_last_run(new_last_run, collection_name)\n",
    "        print(f\"✅ ETL finished. Updated last_run to {new_last_run}\")\n",
    "    else:\n",
    "        print(\"ℹ️ No new data to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_etl(\"milk_purchases\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl_runner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
