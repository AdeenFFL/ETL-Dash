{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7c7eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_8656\\873930971.py:125: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  purchases_df[\"booked_at\"] = pd.to_datetime(purchases_df[\"booked_at\"], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id                created_by  \\\n",
      "0  68c952777f321fa8c7027933  64db7f48966006bb8f0b60d2   \n",
      "1  68c951a77f321fa8c702792f  67bd54c860f986fd420a3ce8   \n",
      "2  68c9509095534ca26d056490  67bd554660f986fd420a3cf2   \n",
      "3  68c94d8a7f321fa8c7027922  674b371f71e97c0db303da9f   \n",
      "4  68c94d0795534ca26d05648b  674b371f71e97c0db303da9f   \n",
      "\n",
      "                     mcc_id             type               supplier_id  \\\n",
      "0  64d637c011fedf702c09ea14  purchase_at_mcc  654c74a68fb27cb1c60bdca2   \n",
      "1  6851122cd8af7017aa040196     mmt_purchase  685219aff848d0028b056535   \n",
      "2  68c0505f06ff4b26ae0b57db     mmt_purchase  68c0530354c7f021f90e5fcf   \n",
      "3  682db2081493ae249f0e7b7c     mmt_purchase  682db2a8187a486a1a0a60b5   \n",
      "4  67c6e242dfcfc989fe0c2ab3     mmt_purchase  67c6e4c0b0848dda750d7672   \n",
      "\n",
      "           supplier_type_id  gross_volume  ts_volume                time  \\\n",
      "0  63b55d49781e0000b4000f03          80.0     79.754 2025-09-16 17:05:00   \n",
      "1  63b55d3e781e0000b4000f02         470.0    446.500 2025-09-16 17:01:00   \n",
      "2  63b55d52781e0000b4000f04        5981.0   6105.681 2025-09-16 16:57:00   \n",
      "3  63d39ac2a80400006c004825        1378.0   1387.328 2025-09-16 16:44:00   \n",
      "4  63d39ac2a80400006c004825         321.0    317.296 2025-09-16 16:41:00   \n",
      "\n",
      "                                               tests  ...  code  \\\n",
      "0  [{'compartment': None, 'compositeKey': '202509...  ...   692   \n",
      "1  [{'compartment': None, 'compositeKey': '202509...  ...  1791   \n",
      "2  [{'compartment': None, 'compositeKey': '202509...  ...  1866   \n",
      "3  [{'compartment': None, 'compositeKey': '202509...  ...  1758   \n",
      "4  [{'compartment': None, 'compositeKey': '202509...  ...  1658   \n",
      "\n",
      "    collection_point_name        area_office_id_mcc is_mcc    latitude  \\\n",
      "0                 432 TDA  63c799e0b57d0000ef002094      1   30.791274   \n",
      "1       Noor Yousaf Dairy  63c79987b57d0000ef002093      0   30.414277   \n",
      "2  HRM Dairies (Pvt) Ltd.  63c79987b57d0000ef002093      0   30.218330   \n",
      "3           Head Satto Ke  67498945ae5614ae29003290      0  30.7515660   \n",
      "4        Gharh Fateh Shah  67498945ae5614ae29003290      0     30.6942   \n",
      "\n",
      "    longitude         area_office_id_ao  area_office_name supplier_type_name  \\\n",
      "0   71.242609  63c799e0b57d0000ef002094            Layyah               MVMC   \n",
      "1   71.635179  63c79987b57d0000ef002093           Shorkot         CDF (Farm)   \n",
      "2   72.972743  63c79987b57d0000ef002093           Shorkot                 LF   \n",
      "3  74.1189210  67498945ae5614ae29003290         Syed Wala               CVMC   \n",
      "4     73.0805  67498945ae5614ae29003290         Syed Wala               CVMC   \n",
      "\n",
      "                       description  \n",
      "0    Mobile Village Milk Collector  \n",
      "1   Commercial Dairy Farmer / Farm  \n",
      "2               Large Dairy Farmer  \n",
      "3  Chilled Village Milk Collection  \n",
      "4  Chilled Village Milk Collection  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "                        _id                     plant  \\\n",
      "0  67113b141150af2c970a7a5c  63907be6b32f0000000031d8   \n",
      "1  683edaa62ebbdf1b5c0f8e17  63907be6b32f0000000031d8   \n",
      "2  683edab75b7b698bad0c6c72  63907be6b32f0000000031d8   \n",
      "3  683edad1ce4c98d9df059598  63907be6b32f0000000031d8   \n",
      "4  683edadfce4c98d9df05959a  63907be6b32f0000000031d8   \n",
      "\n",
      "                 department               area_office  \\\n",
      "0  63907c31b32f0000000031da  63c79839b57d0000ef002091   \n",
      "1  63907c31b32f0000000031da  63c79839b57d0000ef002091   \n",
      "2  63907c31b32f0000000031da  63c79839b57d0000ef002091   \n",
      "3  63907c31b32f0000000031da  63c79839b57d0000ef002091   \n",
      "4  63907c31b32f0000000031da  63c79839b57d0000ef002091   \n",
      "\n",
      "                source_type                  supplier  \\\n",
      "0  63b55d52781e0000b4000f04                      None   \n",
      "1  63b41324992700005e0021c8                      None   \n",
      "2  657ab6172d7c3b4b360cdbc2                      None   \n",
      "3  657ab6172d7c3b4b360cdbc2  6735f48c156610df060bc362   \n",
      "4  657ab6172d7c3b4b360cdbc2  6709fe490ef7f5154d00e9e9   \n",
      "\n",
      "           collection_point  price  volume         wef  \\\n",
      "0                      None  122.0  1300.0  2024-10-19   \n",
      "1                      None  120.0   145.0  2025-06-02   \n",
      "2                      None  113.0  1303.0  2025-06-02   \n",
      "3  64c7e75ff841288c59095640  117.0    70.0  2025-06-02   \n",
      "4  64c7e75ff841288c59095640  117.0    53.0  2025-06-02   \n",
      "\n",
      "               initial_remarks                created_by  status  \\\n",
      "0  Engro & Nestle  competition  64c791337b0a969621047c82       1   \n",
      "1                         None  64c791337b0a969621047c82       1   \n",
      "2                         None  64c791337b0a969621047c82       1   \n",
      "3   Nestle & Engro Competition  64c791337b0a969621047c82       1   \n",
      "4   Nestle & Engro Competition  64c791337b0a969621047c82       1   \n",
      "\n",
      "   update_request   code              updated_at              created_at  \\\n",
      "0               0   3687 2024-10-18 05:37:20.505 2024-10-17 16:28:04.543   \n",
      "1               0  10190 2025-06-05 08:18:47.730 2025-06-03 11:21:10.667   \n",
      "2               0  10190 2025-06-05 08:18:47.763 2025-06-03 11:21:27.287   \n",
      "3               0  10190 2025-06-05 08:18:47.793 2025-06-03 11:21:53.986   \n",
      "4               0  10190 2025-06-05 08:18:47.819 2025-06-03 11:22:07.092   \n",
      "\n",
      "  approved_at              deleted_at remarks  \n",
      "0  18-10-2024 2024-10-18 05:37:20.505     NaN  \n",
      "1  05-06-2025                     NaT     NaN  \n",
      "2  05-06-2025                     NaT     NaN  \n",
      "3  05-06-2025                     NaT     NaN  \n",
      "4  05-06-2025                     NaT     NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_id                         object\n",
       "plant                       object\n",
       "department                  object\n",
       "area_office                 object\n",
       "source_type                 object\n",
       "supplier                    object\n",
       "collection_point            object\n",
       "price                       object\n",
       "volume                      object\n",
       "wef                         object\n",
       "initial_remarks             object\n",
       "created_by                  object\n",
       "status                       int64\n",
       "update_request               int64\n",
       "code                         int64\n",
       "updated_at          datetime64[ns]\n",
       "created_at          datetime64[ns]\n",
       "approved_at                 object\n",
       "deleted_at          datetime64[ns]\n",
       "remarks                     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’° Attaching prices for 11306 purchases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11306/11306 [00:00<00:00, 13415.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pricing complete.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from bson import ObjectId\n",
    "import importlib\n",
    "import pricing\n",
    "importlib.reload(pricing)  # Ensure latest version is used\n",
    "from pricing import attach_prices\n",
    "\n",
    "# -------------------------------\n",
    "# LOGGING SETUP\n",
    "# -------------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"etl_run.log\",  # log file will be created in same folder\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# DB CONNECTIONS\n",
    "# -------------------------------\n",
    "LIVE_CONN_STR = \"mongodb+srv://FFL-User:OoesBMAcjYp4pJGf@fflcluster.2mt2rev.mongodb.net/?retryWrites=true&w=majority&appName=FFLCluster&tls=true\"\n",
    "REPORTING_CONN_STR = \"mongodb://localhost:27017\"\n",
    "\n",
    "live_client = MongoClient(LIVE_CONN_STR)\n",
    "reporting_client = MongoClient(REPORTING_CONN_STR)\n",
    "\n",
    "live_db = live_client[\"ffl\"]\n",
    "reporting_db = reporting_client[\"staging_db\"]\n",
    "\n",
    "# -------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------\n",
    "def to_objectid_safe(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    if isinstance(x, ObjectId):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ObjectId(x)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None \n",
    "\n",
    "def sanitize_datetimes(df):\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: (\n",
    "                    x.to_pydatetime() if isinstance(x, pd.Timestamp) and pd.notna(x)\n",
    "                    else (x if isinstance(x, datetime) else None)\n",
    "                )\n",
    "            )\n",
    "    return df\n",
    "\n",
    "def get_last_run(collection_name):\n",
    "    meta = reporting_db[\"etl_metadata\"].find_one({\"_id\": collection_name})\n",
    "    if meta and \"last_run\" in meta:\n",
    "        ts = meta[\"last_run\"]\n",
    "        if isinstance(ts, pd.Timestamp):\n",
    "            return ts.to_pydatetime()\n",
    "        return ts\n",
    "    return None\n",
    "\n",
    "def update_last_run(timestamp, collection_name):\n",
    "    if pd.isna(timestamp):\n",
    "        safe_ts = None\n",
    "    elif isinstance(timestamp, pd.Timestamp):\n",
    "        safe_ts = timestamp.to_pydatetime()\n",
    "    elif isinstance(timestamp, datetime):\n",
    "        safe_ts = timestamp\n",
    "    else:\n",
    "        safe_ts = None\n",
    "\n",
    "    reporting_db[\"etl_metadata\"].update_one(\n",
    "        {\"_id\": collection_name},\n",
    "        {\"$set\": {\"last_run\": safe_ts}},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "def extract_incremental_purchases(db, collection_name, last_run):\n",
    "    if last_run:\n",
    "        query = {\"created_at\": {\"$gte\": last_run}}\n",
    "    else:\n",
    "        query = {}\n",
    "\n",
    "    fields = {\n",
    "        \"_id\": 1,\n",
    "        \"supplier_id\": 1,\n",
    "        \"supplier_type_id\": 1,\n",
    "        \"mcc_id\": 1,\n",
    "        \"area_office_id\": 1,\n",
    "        \"gross_volume\": 1,\n",
    "        \"ts_volume\": 1,\n",
    "        \"opening_balance\": 1,\n",
    "        \"type\": 1,\n",
    "        \"created_by\": 1,\n",
    "        \"created_at\": 1,\n",
    "        \"booked_at\": 1, \n",
    "        \"time\": 1,\n",
    "        \"serial_number\" : 1, \n",
    "        \"is_planned\": 1, \n",
    "        \"is_exceptional_release\": 1,\n",
    "        \"tests\": 1,\n",
    "        \"plant_id\": 1,\n",
    "        \"price\": 1,\n",
    "        \"cp_id\": 1\n",
    "    }\n",
    "    cursor = db[collection_name].find(query, fields)\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "\n",
    "    for col in [\"mcc_id\", \"supplier_id\", \"supplier_type_id\", \"area_office_id\", \"plant_id\", \"cp_id\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    return df\n",
    "\n",
    "def transform_purchases(purchases_df):\n",
    "    if purchases_df.empty:\n",
    "        return purchases_df\n",
    "    \n",
    "    purchases_df[\"created_at\"] = pd.to_datetime(purchases_df[\"created_at\"], errors=\"coerce\")\n",
    "    purchases_df[\"booked_at\"] = pd.to_datetime(purchases_df[\"booked_at\"], errors=\"ignore\")\n",
    "    purchases_df[\"time\"] = pd.to_datetime(purchases_df[\"time\"], errors=\"coerce\")\n",
    "\n",
    "    null_mask = purchases_df[\"booked_at\"].isna()\n",
    "    if null_mask.any():\n",
    "        logging.info(f\"Replacing {int(null_mask.sum())} null booked_at values with time\")\n",
    "        purchases_df.loc[null_mask, \"booked_at\"] = purchases_df.loc[null_mask, \"time\"]\n",
    "\n",
    "    suppliers_df = pd.DataFrame(list(live_db[\"suppliers\"].find({})))\n",
    "    collection_points_df = pd.DataFrame(list(live_db[\"collection_points\"].find({}, {\n",
    "        \"_id\": 1, \"name\": 1, \"area_office_id\": 1, \"status\": 1,\n",
    "        \"is_mcc\": 1, \"latitude\": 1, \"longitude\": 1, \"address\": 1\n",
    "    })))\n",
    "    area_offices_df = pd.DataFrame(list(live_db[\"area_offices\"].find({}, {\"_id\": 1, \"name\": 1})))\n",
    "    supplier_types_df = pd.DataFrame(list(live_db[\"supplier_types\"].find({}, {\"_id\": 1, \"name\": 1, \"description\": 1})))\n",
    "\n",
    "    # Convert IDs\n",
    "    purchases_df[\"_id\"] = purchases_df[\"_id\"].apply(to_objectid_safe)\n",
    "    suppliers_df[\"_id\"] = suppliers_df[\"_id\"].apply(to_objectid_safe)\n",
    "    collection_points_df[\"_id\"] = collection_points_df[\"_id\"].apply(to_objectid_safe)\n",
    "    collection_points_df[\"area_office_id\"] = collection_points_df[\"area_office_id\"].apply(to_objectid_safe)\n",
    "    area_offices_df[\"_id\"] = area_offices_df[\"_id\"].apply(to_objectid_safe)\n",
    "    supplier_types_df[\"_id\"] = supplier_types_df[\"_id\"].apply(to_objectid_safe)\n",
    "\n",
    "    suppliers_df[\"supplier_type_id\"] = suppliers_df[\"supplier_type_id\"].apply(to_objectid_safe)\n",
    "    purchases_df[\"supplier_id\"] = purchases_df[\"supplier_id\"].apply(to_objectid_safe)\n",
    "    purchases_df[\"mcc_id\"]=purchases_df[\"mcc_id\"].fillna(purchases_df[\"cp_id\"])\n",
    "    purchases_df[\"mcc_id\"] = purchases_df[\"mcc_id\"].apply(to_objectid_safe)\n",
    "    purchases_df[\"supplier_type_id\"] = purchases_df[\"supplier_type_id\"].apply(to_objectid_safe)\n",
    "\n",
    "    # Select relevant columns\n",
    "    suppliers_df  = suppliers_df[[\"_id\", \"name\", \"supplier_type_id\", \"source\", \"area_office\", \"code\"]]\n",
    "    collection_points_df = collection_points_df[[\"_id\", \"name\", \"area_office_id\", \"is_mcc\", \"latitude\", \"longitude\"]]\n",
    "    area_offices_df = area_offices_df[[\"_id\", \"name\"]]\n",
    "    supplier_types_df = supplier_types_df[[\"_id\", \"name\", \"description\"]]\n",
    "\n",
    "    # Joins\n",
    "    purchases_df = purchases_df.merge(\n",
    "        suppliers_df.rename(columns={\"_id\": \"supplier_id\", \"name\": \"supplier_name\"}),\n",
    "        on=\"supplier_id\", how=\"left\", suffixes=(\"\", \"_sup\")\n",
    "    )\n",
    "    purchases_df = purchases_df.merge(\n",
    "        collection_points_df.rename(columns={\"_id\": \"mcc_id\", \"name\": \"collection_point_name\"}),\n",
    "        on=\"mcc_id\", how=\"left\", suffixes=(\"\", \"_mcc\")\n",
    "    )\n",
    "    purchases_df = purchases_df.merge(\n",
    "        area_offices_df.rename(columns={\"_id\": \"area_office_id\", \"name\": \"area_office_name\"}),\n",
    "        left_on=\"area_office_id_mcc\",right_on=\"area_office_id\", how=\"left\", suffixes=(\"\", \"_ao\")\n",
    "    )\n",
    "    purchases_df = purchases_df.merge(\n",
    "        supplier_types_df.rename(columns={\"_id\": \"supplier_type_id\", \"name\": \"supplier_type_name\"}),\n",
    "        on=\"supplier_type_id\", how=\"left\"\n",
    "    )\n",
    "    purchases_df[\"area_office_id_ao\"]=purchases_df[\"area_office_id_ao\"].fillna(purchases_df[\"area_office_id\"])\n",
    "\n",
    "    return purchases_df\n",
    "\n",
    "def load_to_reporting(df, collection_name):\n",
    "    if df.empty:\n",
    "        logging.info(\"âš ï¸ Nothing to load\")\n",
    "        return\n",
    "\n",
    "    df = sanitize_datetimes(df)\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    df = df[df[\"_id\"].notnull()]\n",
    "    ops = []\n",
    "    for _, row in df.iterrows():\n",
    "        doc = row.to_dict()\n",
    "        _id = doc.pop(\"_id\")\n",
    "        ops.append(UpdateOne({\"_id\": _id}, {\"$set\": doc}, upsert=True))\n",
    "\n",
    "    result = reporting_db[collection_name].bulk_write(ops, ordered=False)\n",
    "    logging.info(f\"âœ… Load completed: matched={result.matched_count}, modified={result.modified_count}, upserted={len(result.upserted_ids)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# MAIN ETL RUN\n",
    "# -------------------------------\n",
    "def run_etl(collection_name):\n",
    "    start_time = time.time()\n",
    "    logging.info(f\"ETL started for {collection_name}\")\n",
    "\n",
    "    last_run = get_last_run(collection_name)\n",
    "    logging.info(f\"Last run checkpoint: {last_run}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Extract\n",
    "        if collection_name == \"milk_purchases\":\n",
    "            relevant_df = extract_incremental_purchases(live_db, collection_name, last_run)\n",
    "            logging.info(f\"Extracted {len(relevant_df)} new/updated purchases\")\n",
    "        else:\n",
    "            logging.warning(f\"No extraction defined for collection: {collection_name}\")\n",
    "            relevant_df = pd.DataFrame()\n",
    "\n",
    "        # 2. Transform\n",
    "        if collection_name == \"milk_purchases\":\n",
    "            transformed_df = transform_purchases(relevant_df)\n",
    "            logging.info(f\"Transformed {len(transformed_df)} rows\")\n",
    "            print(transformed_df.head())\n",
    "            prices_df = pd.DataFrame(list(live_db[\"prices\"].find({\"status\": 1})))\n",
    "            print(prices_df.head())\n",
    "            archived_prices_df = pd.DataFrame(list(live_db[\"archieved_prices\"].find({\"status\": 1})))\n",
    "\n",
    "            for df in [prices_df, archived_prices_df]:\n",
    "                for col in [\"plant\", \"source_type\", \"area_office\", \"supplier\", \"collection_point\"]:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = df[col].apply(to_objectid_safe)\n",
    "\n",
    "            transformed_df = attach_prices(transformed_df, prices_df, archived_prices_df)\n",
    "            logging.info(\"ðŸ’° Prices attached\")\n",
    "\n",
    "        # 3. Load\n",
    "        load_to_reporting(transformed_df, \"fact_\"+collection_name)\n",
    "\n",
    "        # 4. Update checkpoint\n",
    "        if not relevant_df.empty:\n",
    "            new_last_run = relevant_df[\"created_at\"].max()\n",
    "            update_last_run(new_last_run, collection_name)\n",
    "            logging.info(f\"Updated last_run checkpoint to {new_last_run}\")\n",
    "        else:\n",
    "            logging.info(\"No new data to process\")\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"ETL finished for {collection_name} in {elapsed:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        logging.error(f\"ETL failed for {collection_name} after {elapsed:.2f} seconds. Error: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_etl(\"milk_purchases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1352d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting facts: 354979 rows\n",
      "Staging facts: 421727 rows\n",
      "Merged: 354979 rows\n",
      "Mismatches: 83\n",
      "\n",
      "Sample mismatches:\n",
      "                             _id               purchase_id             type  \\\n",
      "342043  68a1ef79879eb127380f8604  68a1ef79879eb127380f8604  purchase_at_mcc   \n",
      "342057  68a1fbc471edb9785c00abe2  68a1fbc471edb9785c00abe2  purchase_at_mcc   \n",
      "342060  68a1fe1471edb9785c00abea  68a1fe1471edb9785c00abea  purchase_at_mcc   \n",
      "342099  68a28c9cb98b43585905eac8  68a28c9cb98b43585905eac8  purchase_at_mcc   \n",
      "342102  68a28cb7b98b43585905eacc  68a28cb7b98b43585905eacc  purchase_at_mcc   \n",
      "342103  68a28cd55721c6d98b0d4987  68a28cd55721c6d98b0d4987  purchase_at_mcc   \n",
      "342108  68a290d5fefebffc250ef569  68a290d5fefebffc250ef569  purchase_at_mcc   \n",
      "342145  68a29de158623940f90a1be2  68a29de158623940f90a1be2  purchase_at_mcc   \n",
      "342207  68a2b37ca34d21dca60f8d0a  68a2b37ca34d21dca60f8d0a  purchase_at_mcc   \n",
      "342233  68a2bc667416a9ef2306bda8  68a2bc667416a9ef2306bda8  purchase_at_mcc   \n",
      "\n",
      "        price  base_price  price_diff  \n",
      "342043  135.0       127.0        -8.0  \n",
      "342057  130.0       134.0         4.0  \n",
      "342060  130.0       134.0         4.0  \n",
      "342099  130.0       135.0         5.0  \n",
      "342102  130.0       135.0         5.0  \n",
      "342103  130.0       135.0         5.0  \n",
      "342108  135.0       127.0        -8.0  \n",
      "342145  130.0       135.0         5.0  \n",
      "342207  130.0       135.0         5.0  \n",
      "342233  134.0       141.0         7.0  \n",
      "\n",
      "Results saved to price_comparison.csv and price_mismatches.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from bson import ObjectId\n",
    "\n",
    "def safe_to_objectid(val):\n",
    "    \"\"\"Convert 24-char hex string to ObjectId, otherwise return as-is.\"\"\"\n",
    "    if isinstance(val, str) and len(val) == 24:\n",
    "        try:\n",
    "            return ObjectId(val)\n",
    "        except Exception:\n",
    "            return val\n",
    "    return val\n",
    "\n",
    "def main():\n",
    "    # 1. Connect to MongoDB\n",
    "    LIVE_CONN_STR = \"mongodb+srv://FFL-User:OoesBMAcjYp4pJGf@fflcluster.2mt2rev.mongodb.net/?retryWrites=true&w=majority&appName=FFLCluster&tls=true\"\n",
    "    REPORTING_CONN_STR = \"mongodb://localhost:27017\"\n",
    "\n",
    "    live_client = MongoClient(LIVE_CONN_STR)\n",
    "    reporting_client = MongoClient(REPORTING_CONN_STR)\n",
    "\n",
    "    live_db = live_client[\"ffl\"]\n",
    "    reporting_db = reporting_client[\"staging_db\"]\n",
    "\n",
    "    # DB + collection handles\n",
    "    reporting_col = live_client[\"ffl\"][\"milk_purchase_reporting_facts\"]\n",
    "    staging_col = reporting_client[\"staging_db\"][\"fact_milk_purchases\"]\n",
    "\n",
    "    # 2. Load into DataFrames\n",
    "    reporting_df = pd.DataFrame(\n",
    "        list(reporting_col.find({}, {\"purchase_id\": 1, \"base_price\": 1, \"_id\": 0}))\n",
    "    )\n",
    "    staging_df = pd.DataFrame(\n",
    "        list(staging_col.find({}))  # fetch EVERYTHING\n",
    "    )\n",
    "\n",
    "    print(f\"Reporting facts: {len(reporting_df)} rows\")\n",
    "    print(f\"Staging facts: {len(staging_df)} rows\")\n",
    "\n",
    "    # 3. Convert purchase_id into ObjectId (if string hex)\n",
    "    reporting_df[\"purchase_id\"] = reporting_df[\"purchase_id\"].apply(safe_to_objectid)\n",
    "\n",
    "    # 4. Merge on purchase_id vs _id\n",
    "    merged = staging_df.merge(\n",
    "        reporting_df,\n",
    "        left_on=\"_id\",\n",
    "        right_on=\"purchase_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    print(f\"Merged: {len(merged)} rows\")\n",
    "\n",
    "    if merged.empty:\n",
    "        print(\"âš ï¸ No matches found! Check if IDs are in different formats.\")\n",
    "        return\n",
    "\n",
    "    # 5. Ensure numeric types for price comparison\n",
    "    merged[\"price\"] = pd.to_numeric(merged[\"price\"], errors=\"coerce\")\n",
    "    merged[\"base_price\"] = pd.to_numeric(merged[\"base_price\"], errors=\"coerce\")\n",
    "\n",
    "    # 6. Compute difference\n",
    "    merged[\"price_diff\"] = merged[\"base_price\"] - merged[\"price\"]\n",
    "\n",
    "    # 7. Find mismatches\n",
    "    mismatches = merged[merged[\"price_diff\"].abs() > 0.0001]  # tolerance\n",
    "\n",
    "    print(f\"Mismatches: {len(mismatches)}\")\n",
    "\n",
    "    print(\"\\nSample mismatches:\")\n",
    "    mismatch_cols = [\"_id\", \"purchase_id\", \"type\", \"booked_date\", \"price\", \"base_price\", \"price_diff\"]\n",
    "    available_cols = [col for col in mismatch_cols if col in mismatches.columns]\n",
    "    print(mismatches[available_cols].head(10))\n",
    "\n",
    "    # 8. Save results â€” now includes *all staging columns*\n",
    "    merged.to_csv(\"price_comparison.csv\", index=False)\n",
    "    mismatches.to_csv(\"price_mismatches.csv\", index=False)\n",
    "\n",
    "    print(\"\\nResults saved to price_comparison.csv and price_mismatches.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl_runner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
